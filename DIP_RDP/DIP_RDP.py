# Copyright University College London 2021
# Author: Alexander Whitehead, Institute of Nuclear Medicine, UCL
# For internal research only.


import gc
import os
import re
import shutil
import random
import numpy as np
import scipy.constants
import scipy.stats
import scipy.ndimage
import tensorflow as tf
import tensorflow.keras as k
from numba import cuda
import pickle
import matplotlib.pyplot as plt
import nibabel as nib
import gzip


reproducible_bool = True

if reproducible_bool:
    # Seed value (can actually be different for each attribution step)
    seed_value = 0

    # 1. Set "PYTHONHASHSEED" environment variable at a fixed value
    os.environ["PYTHONHASHSEED"] = str(seed_value)

    # 2. Set `python` built-in pseudo-random generator at a fixed value
    random.seed(seed_value)

    # 3. Set `numpy` pseudo-random generator at a fixed value
    np.random.seed(seed_value)

    # 4. Set `tensorflow` pseudo-random generator at a fixed value
    tf.random.set_seed(seed_value)

    # 5. Configure a new global `tensorflow` session
    # ONLY WORKS WITH TF V1
    # session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
    # sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
    # tf.compat.v1.keras.backend.set_session(sess)
else:
    random.seed()

device = cuda.get_current_device()

# physical_devices = tf.config.list_physical_devices("GPU")

# try:
#     tf.config.experimental.set_memory_growth(physical_devices[0], True)
# except:
    # Invalid device or cannot modify virtual devices once initialized.
#     pass

float_sixteen_bool = True  # set the network to use float16 data
cpu_bool = False  # if using CPU, set to true: disables mixed precision computation

# mixed precision float16 computation allows the network to use both float16 and float32 where necessary,
# this improves performance on the GPU.
if float_sixteen_bool and not cpu_bool:
    policy = k.mixed_precision.Policy("mixed_float16")
    k.mixed_precision.set_global_policy(policy)
else:
    policy = k.mixed_precision.Policy(tf.dtypes.float32.name)
    k.mixed_precision.set_global_policy(policy)


import parameters
import preprocessing
import architecture
import losses


data_path = None
output_path = None


# https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside
def atoi(string):
    print("atoi")

    return int(string) if string.isdigit() else string


# https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside
def human_sorting(string):
    print("human_sorting")

    return [atoi(c) for c in re.split(r'(\d+)', string)]


def get_data_windows(data):
    print("get_data_windows")

    axial_size = data.shape[-1]

    windowed_full_input_axial_size = None

    if axial_size > parameters.data_window_size:
        if parameters.data_window_bool:
            number_of_windows = int(np.ceil(axial_size / parameters.data_window_size)) + 1
            number_of_overlaps = number_of_windows - 1

            overlap_size = \
                int(np.ceil((((parameters.data_window_size * number_of_windows) - axial_size) / number_of_overlaps)))
            overlap_index = int(np.ceil(parameters.data_window_size - overlap_size))

            windowed_full_input_axial_size = \
                (number_of_windows * parameters.data_window_size) - (number_of_overlaps * overlap_size)

            data = np.squeeze(preprocessing.data_upsample_pad([np.expand_dims(np.expand_dims(data, 0), -1)], "numpy",
                                                              (data.shape[0], data.shape[1],
                                                               windowed_full_input_axial_size)))

            current_data = []

            for i in range(number_of_windows):
                current_overlap_index = overlap_index * i

                current_data.append(
                    data[:, :, current_overlap_index:current_overlap_index + parameters.data_window_size])

            data = current_data
        else:
            data_centre = int(axial_size / 2.0)

            data_upper_window_size = int(parameters.data_window_size / 2.0)
            data_lower_window_size = data_upper_window_size

            if data_upper_window_size + data_lower_window_size < parameters.data_window_size:
                data_upper_window_size = data_upper_window_size + 1

            data = [data[:, :, data_centre - data_upper_window_size:data_centre + data_lower_window_size]]
    else:
        data = [data]

    data = np.asarray(data)

    if windowed_full_input_axial_size is None:
        windowed_full_input_axial_size = data.shape[-1]

    return data, windowed_full_input_axial_size


def normalise_voxel_sizes(data, voxel_sizes):
    print("normalise_voxel_sizes")

    min_voxel_sizes = np.min(voxel_sizes)
    data_shape = data.shape

    data = \
        np.squeeze(preprocessing.data_upsample([data], "numpy",
                                               (int(np.round((data_shape[0] / min_voxel_sizes) * voxel_sizes[0])),
                                                int(np.round((data_shape[1] / min_voxel_sizes) * voxel_sizes[1])),
                                                int(np.round((data_shape[2] / min_voxel_sizes) * voxel_sizes[2]))))[0])

    return data


def get_train_data():
    print("get_train_data")

    y_path = "{0}/y/".format(data_path)

    y_files = os.listdir(y_path)
    y_files.sort(key=human_sorting)
    y_files = ["{0}{1}".format(y_path, s) for s in y_files]

    example_data = nib.load(y_files[0])
    voxel_sizes = example_data.get_header().get_zooms()

    data_mask_path = "{0}/data_mask/".format(data_path)

    if os.path.exists(data_mask_path):
        data_mask_files = os.listdir(data_mask_path)
        data_mask_files.sort(key=human_sorting)
        data_mask_files = ["{0}{1}".format(data_mask_path, s) for s in data_mask_files]

        data_mask = []

        data_mask_train_output_path = "{0}/data_mask_train".format(output_path)

        if not os.path.exists(data_mask_train_output_path):
            os.makedirs(data_mask_train_output_path, mode=0o770)

        for i in range(len(data_mask_files)):
            current_array = nib.load(data_mask_files[i]).get_data()

            current_array = normalise_voxel_sizes(current_array, voxel_sizes)
            current_array, _ = get_data_windows(current_array)

            current_data_mask_train_path = "{0}/{1}.npy.gz".format(data_mask_train_output_path, str(i))

            with gzip.GzipFile(current_data_mask_train_path, "w") as file:
                np.save(file, current_array)

            data_mask.append(current_data_mask_train_path)

        data_mask = np.asarray(data_mask)
    else:
        data_mask = None

    loss_mask_path = "{0}/loss_mask/".format(data_path)

    loss_mask_train_output_path = "{0}/loss_mask_train".format(output_path)

    if not os.path.exists(loss_mask_train_output_path):
        os.makedirs(loss_mask_train_output_path, mode=0o770)

    if os.path.exists(loss_mask_path):
        loss_mask = []

        loss_mask_files = os.listdir(loss_mask_path)
        loss_mask_files.sort(key=human_sorting)
        loss_mask_files = ["{0}{1}".format(loss_mask_path, s) for s in loss_mask_files]

        for i in range(len(loss_mask_files)):
            current_array = nib.load(loss_mask_files[i]).get_data()

            current_array = normalise_voxel_sizes(current_array, voxel_sizes)
            current_array, _ = get_data_windows(current_array)

            current_loss_mask_train_path = "{0}/{1}.npy.gz".format(loss_mask_train_output_path, str(i))

            with gzip.GzipFile(current_loss_mask_train_path, "w") as file:
                np.save(file, current_array)

            loss_mask.append(current_loss_mask_train_path)
    else:
        loss_mask = None

    x = []
    y = []

    x_train_output_path = "{0}/x_train".format(output_path)

    if not os.path.exists(x_train_output_path):
        os.makedirs(x_train_output_path, mode=0o770)

    y_train_output_path = "{0}/y_train".format(output_path)

    if not os.path.exists(y_train_output_path):
        os.makedirs(y_train_output_path, mode=0o770)

    full_current_shape = None
    windowed_full_input_axial_size = None
    current_shape = None
    gaussian_noise = None

    for i in range(len(y_files)):
        current_volume = nib.load(y_files[i])
        current_array = current_volume.get_data()

        current_array = normalise_voxel_sizes(current_array, voxel_sizes)

        if parameters.data_window_bool:
            if full_current_shape is None:
                full_current_shape = current_array.shape

        current_array, windowed_full_input_axial_size = get_data_windows(current_array)

        if full_current_shape is None:
            full_current_shape = current_array.shape[1:]

        if current_shape is None:
            current_shape = current_array[0].shape

        current_y_train_path = "{0}/{1}.npy.gz".format(y_train_output_path, str(i))

        with gzip.GzipFile(current_y_train_path, "w") as file:
            np.save(file, current_array)

        y.append(current_y_train_path)

        if parameters.data_input_bool:
            if parameters.data_gaussian_smooth_sigma_xy > 0.0 or parameters.data_gaussian_smooth_sigma_z > 0.0:
                if loss_mask is not None:
                    with gzip.GzipFile(loss_mask[i], "r") as file:
                        current_loss_mask = np.load(file)

                    current_array = current_array * current_loss_mask

                current_array = scipy.ndimage.gaussian_filter(current_array,
                                                              sigma=(0.0,
                                                                     parameters.data_gaussian_smooth_sigma_xy,
                                                                     parameters.data_gaussian_smooth_sigma_xy,
                                                                     parameters.data_gaussian_smooth_sigma_z),
                                                              mode="edge")

                current_array, _ = preprocessing.data_preprocessing(current_array, "numpy")
        else:
            current_array = np.zeros(current_array.shape)

        if parameters.input_gaussian_weight > 0.0:
            if parameters.data_input_bool:
                current_array, _ = preprocessing.data_preprocessing(current_array, "numpy")

            if gaussian_noise is None:
                gaussian_update_path = "{0}/gaussian.npy.gk".format(parameters.gaussian_path)

                if os.path.exists(gaussian_update_path):
                    print("Previous gaussian found!")

                    with gzip.GzipFile(gaussian_update_path, "r") as file:
                        gaussian_noise = np.load(file)
                else:
                    gaussian_noise = np.random.normal(size=current_array.shape)

                    if data_mask is not None:
                        with gzip.GzipFile(data_mask[i], "r") as file:
                            current_data_mask = np.load(file)

                        data_mask_gaussian_noise = gaussian_noise * current_data_mask
                    else:
                        data_mask_gaussian_noise = gaussian_noise

                    if loss_mask is not None:
                        with gzip.GzipFile(loss_mask[i], "r") as file:
                            current_loss_mask = np.load(file)

                        loss_mask_current_array = gaussian_noise * ((current_loss_mask * -1.0) + 1.0)
                    else:
                        loss_mask_current_array = 0.0

                    gaussian_noise = data_mask_gaussian_noise + loss_mask_current_array

                    gaussian_update_path = "{0}/gaussian.npy.gk".format(parameters.output_path)

                    with gzip.GzipFile(gaussian_update_path, "w") as file:
                        np.save(file, gaussian_noise)

            current_array = ((current_array + (parameters.input_gaussian_weight * gaussian_noise)) /
                             (1.0 + parameters.input_gaussian_weight))

        current_x_train_path = "{0}/{1}.npy.gz".format(x_train_output_path, str(i))

        with gzip.GzipFile(current_x_train_path, "w") as file:
            np.save(file, current_array)

        x.append(current_x_train_path)

    gt_path = "{0}/gt/".format(data_path)

    if os.path.exists(gt_path):
        gt_files = os.listdir(gt_path)
        gt_files.sort(key=human_sorting)
        gt_files = ["{0}{1}".format(gt_path, s) for s in gt_files]

        gt = []

        gt_train_output_path = "{0}/gt_train".format(output_path)

        if not os.path.exists(gt_train_output_path):
            os.makedirs(gt_train_output_path, mode=0o770)

        for i in range(len(gt_files)):
            current_array = nib.load(gt_files[i]).get_data()

            current_array = normalise_voxel_sizes(current_array, voxel_sizes)
            current_array, _ = get_data_windows(current_array)

            if data_mask is not None:
                with gzip.GzipFile(data_mask[i], "r") as file:
                    current_data_mask = np.load(file)

                current_array = current_array * current_data_mask

            current_gt_train_path = "{0}/{1}.npy.gz".format(gt_train_output_path, str(i))

            with gzip.GzipFile(current_gt_train_path, "w") as file:
                np.save(file, current_array)

            gt.append(current_gt_train_path)

        gt = np.asarray(gt)
    else:
        gt = None

    if loss_mask is None:
        loss_mask = []

        for i in range(len(y_files)):
            with gzip.GzipFile(y[i], "r") as file:
                current_array = np.ones(np.load(file).shape)

            current_loss_mask_train_path = "{0}/{1}.npy.gz".format(loss_mask_train_output_path, str(i))

            with gzip.GzipFile(current_loss_mask_train_path, "w") as file:
                np.save(file, current_array)

            loss_mask.append(current_loss_mask_train_path)

    return x, y, example_data, full_current_shape, windowed_full_input_axial_size, current_shape, gt, data_mask, loss_mask


def get_preprocessed_train_data():
    print("get_preprocessed_train_data")

    x, y, example_data, full_input_shape, windowed_full_input_axial_size, window_input_shape, gt, data_mask, loss_mask = \
        get_train_data()

    x = preprocessing.data_upsample_pad(x, "path")
    y = preprocessing.data_upsample_pad(y, "path")

    if gt is not None:
        gt = preprocessing.data_upsample_pad(gt, "path")

    if data_mask is not None:
        data_mask = preprocessing.data_upsample_pad(data_mask, "path")

    if loss_mask is not None:
        loss_mask = preprocessing.data_upsample_pad(loss_mask, "path")

    x, _ = preprocessing.data_preprocessing(x, "path")
    y, preprocessing_steps = preprocessing.data_preprocessing(y, "path")

    if gt is not None:
        gt, _ = preprocessing.data_preprocessing(gt, "path")

    return x, y, example_data, preprocessing_steps, full_input_shape, windowed_full_input_axial_size, window_input_shape, gt, data_mask, loss_mask


# https://stackoverflow.com/questions/43137288/how-to-determine-needed-memory-of-keras-model
def get_model_memory_usage(batch_size, model):
    print("get_model_memory_usage")

    shapes_mem_count = 0
    internal_model_mem_count = 0

    for l in model.layers:
        layer_type = l.__class__.__name__

        if layer_type == "Model":
            internal_model_mem_count += get_model_memory_usage(batch_size, l)

        single_layer_mem = 1
        out_shape = l.output_shape

        if type(out_shape) is list:
            out_shape = out_shape[0]

        for s in out_shape:
            if s is None:
                continue

            single_layer_mem = single_layer_mem * s

        shapes_mem_count = shapes_mem_count + single_layer_mem

    trainable_count = np.sum([k.backend.count_params(p) for p in model.trainable_weights])
    non_trainable_count = np.sum([k.backend.count_params(p) for p in model.non_trainable_weights])

    number_size = 4.0

    if k.backend.floatx() == "float16":
        number_size = 2.0

    if k.backend.floatx() == "float64":
        number_size = 8.0

    total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)
    gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count

    return gbytes


def get_bayesian_train_prediction(model, x_train_iteration):
    x_prediction_1 = model(x_train_iteration, training=True)

    gc.collect()
    k.backend.clear_session()

    x_prediction = tf.math.reduce_mean([x_prediction_1], axis=0)

    x_prediction_uncertainty = tf.cast(tf.math.reduce_mean(tf.math.reduce_std([x_prediction_1], axis=0)),
                                       dtype=tf.float32)

    return x_prediction, x_prediction_uncertainty


def train_backup(model, optimiser, loss_list, previous_model_weight_list, previous_optimiser_weight_list,
                 current_loss_increase_patience):
    print("train_backup")

    print("WARNING: Loss increased above threshold or has become NaN; backing up...")

    tape = None

    if len(loss_list) > 1:
        loss_list.pop()
        previous_model_weight_list.pop()
        previous_optimiser_weight_list.pop()

    with open(previous_model_weight_list[-1], "rb") as file:
        model.set_weights(pickle.load(file))

    try:
        with open(previous_optimiser_weight_list[-1], "rb") as file:
            optimiser.set_weights(pickle.load(file))
    except:
        optimiser = architecture.get_optimiser()

    current_loss_increase_patience = current_loss_increase_patience + 1

    if len(loss_list) > 1:
        loss_list.pop()
        previous_model_weight_list.pop()
        previous_optimiser_weight_list.pop()

    return tape, model, optimiser, loss_list, previous_model_weight_list, previous_optimiser_weight_list, current_loss_increase_patience


def train_step(model, optimiser, loss, x_train_iteration, y_train_iteration, loss_mask_train_iteration, loss_list,
               previous_model_weight_list, previous_optimiser_weight_list, model_output_path):
    current_loss_increase_patience = 0

    while True:
        previous_model_weight_list.append("{0}/model_{1}.pkl".format(model_output_path, str(len(previous_model_weight_list))))

        with open(previous_model_weight_list[-1], "wb") as file:
            pickle.dump(model.get_weights(), file)

        previous_optimiser_weight_list.append("{0}/optimiser_{1}.pkl".format(model_output_path, str(len(previous_optimiser_weight_list))))

        with open(previous_optimiser_weight_list[-1], "wb") as file:
            pickle.dump(optimiser.get_weights(), file)

        x_train_iteration_jitter, y_train_iteration_jitter, loss_mask_train_iteration = \
            preprocessing.introduce_jitter(x_train_iteration, y_train_iteration, loss_mask_train_iteration)

        # Open a GradientTape to record the operations run
        # during the forward pass, which enables auto-differentiation.
        with tf.GradientTape() as tape:
            tape.reset()

            # Run the forward pass of the layer.
            # The operations that the layer applies
            # to its inputs are going to be recorded
            # on the GradientTape.
            logits, uncertainty = get_bayesian_train_prediction(model, x_train_iteration_jitter)  # Logits for this minibatch

            current_y_train_iteration = y_train_iteration * loss_mask_train_iteration
            logits = logits * loss_mask_train_iteration

            # Compute the loss value for this minibatch.
            current_loss = tf.math.reduce_sum([loss(current_y_train_iteration, logits),
                                               parameters.scale_weight *
                                               losses.log_cosh_loss(tf.math.reduce_mean(y_train_iteration_jitter),
                                                                    tf.math.reduce_mean(logits)),
                                               parameters.uncertainty_weight * uncertainty,
                                               parameters.kernel_regulariser_weight *
                                               tf.math.reduce_mean(model.losses[::3]),
                                               parameters.activity_regulariser_weight *
                                               tf.math.reduce_mean(model.losses[1::3]),
                                               parameters.prelu_regulariser_weight *
                                               tf.math.reduce_mean(model.losses[2::3])])

        loss_list.append(current_loss)

        if np.isnan(loss_list[-1].numpy()) or np.isinf(loss_list[-1].numpy()):
            tape, model, optimiser, loss_list, previous_model_weight_list, previous_optimiser_weight_list, current_loss_increase_patience = \
                train_backup(model, optimiser, loss_list, previous_model_weight_list, previous_optimiser_weight_list,
                             current_loss_increase_patience)

            continue

        if len(loss_list) > 1:
            current_patience = parameters.patience

            if current_patience < 2:
                current_patience = 2

            loss_gradient = np.gradient(loss_list[-current_patience:])[-1]

            if not np.allclose(loss_gradient, np.zeros(loss_gradient.shape), rtol=0.0, atol=parameters.plateau_cutoff):
                if not (loss_list[-1] * (parameters.backtracking_weight_percentage / 100.0) < loss_list[-2]):
                    if current_loss_increase_patience >= parameters.patience:
                        print("WARNING: Loss increased above threshold; patience reached, allowing anyway!")

                        break
                    else:
                        tape, model, optimiser, loss_list, previous_model_weight_list, previous_optimiser_weight_list, current_loss_increase_patience = \
                            train_backup(model, optimiser, loss_list, previous_model_weight_list,
                                         previous_optimiser_weight_list, current_loss_increase_patience)
                else:
                    break
            else:
                break
        else:
            break

    gc.collect()
    k.backend.clear_session()

    # Use the gradient tape to automatically retrieve
    # the gradients of the trainable variables with respect to the loss.
    grads = tape.gradient(current_loss, model.trainable_weights)

    del tape

    # Run one step of gradient descent by updating
    # the value of the variables to minimize the loss.
    optimiser.apply_gradients(zip(grads, model.trainable_weights))

    gc.collect()
    k.backend.clear_session()

    return model, loss_list, uncertainty, previous_model_weight_list, previous_optimiser_weight_list


def get_bayesian_test_prediction(model, x_train_iteration, bayesian_iteration, bayesian_bool):
    print("get_bayesian_test_prediction")

    x_prediction_list = []

    if not bayesian_bool:
        bayesian_iteration = 1

    for i in range(bayesian_iteration):
        x_prediction_list.append(model(x_train_iteration, training=bayesian_bool))

        gc.collect()
        k.backend.clear_session()

    x_prediction = tf.math.reduce_mean(x_prediction_list, axis=0)

    x_prediction_uncertainty_volume = tf.math.reduce_std(x_prediction_list, axis=0)
    x_prediction_uncertainty = tf.math.reduce_mean(x_prediction_uncertainty_volume)

    return x_prediction, x_prediction_uncertainty, x_prediction_uncertainty_volume


def test_step(model, loss, x_train_iteration, y_train_iteration, loss_mask_train_iteration, evaluation_loss_list):
    print("test_step")

    x_prediction, x_prediction_uncertainty, x_prediction_uncertainty_volume = \
        get_bayesian_test_prediction(model, x_train_iteration, parameters.bayesian_iterations,
                                     parameters.bayesian_test_bool)

    current_y_train_iteration = y_train_iteration * loss_mask_train_iteration
    current_x_prediction = x_prediction * loss_mask_train_iteration

    evaluation_loss_list.append(loss(current_y_train_iteration, current_x_prediction))

    current_accuracy = [losses.correlation_coefficient_accuracy(current_y_train_iteration, current_x_prediction),
                        losses.scale_accuracy(current_y_train_iteration, current_x_prediction)]

    return x_prediction, evaluation_loss_list, x_prediction_uncertainty, x_prediction_uncertainty_volume, current_accuracy


def test_backup(model, optimiser, loss_list, evaluation_loss_list, previous_model_weight_list,
                previous_optimiser_weight_list):
    print("train_backup")

    print("WARNING: Accuracy has become NaN; backing up...")

    if len(loss_list) > 1:
        loss_list.pop()
        evaluation_loss_list.pop()
        previous_model_weight_list.pop()
        previous_optimiser_weight_list.pop()

    with open(previous_model_weight_list[-1], "rb") as file:
        model.set_weights(pickle.load(file))

    try:
        with open(previous_optimiser_weight_list[-1], "rb") as file:
            optimiser.set_weights(pickle.load(file))
    except:
        optimiser = architecture.get_optimiser()

    if len(loss_list) > 1:
        loss_list.pop()
        evaluation_loss_list.pop()
        previous_model_weight_list.pop()
        previous_optimiser_weight_list.pop()

    return model, optimiser, loss_list, evaluation_loss_list, previous_model_weight_list, previous_optimiser_weight_list


def output_window_predictions(x_prediction, y_train_iteration, gt_prediction, loss_mask_train_iteration,
                              x_prediction_uncertainty_volume, preprocessing_steps, window_input_shape,
                              current_output_path, j):
    print("output_window_predictions")

    x_prediction, _ = preprocessing.data_preprocessing([x_prediction], "numpy", preprocessing_steps)
    x_prediction = np.squeeze(preprocessing.data_downsampling_crop([x_prediction], "numpy", window_input_shape)[0])

    x_prediction_uncertainty_volume = np.squeeze(preprocessing.data_downsampling_crop([x_prediction_uncertainty_volume],
                                                                                      "numpy", window_input_shape)[0])

    inverse_loss_mask_train_iteration = (loss_mask_train_iteration * -1.0) + 1.0

    if tf.reduce_sum(tf.cast(inverse_loss_mask_train_iteration, dtype=tf.float32)) > 0.0:
        inverse_loss_mask_train_iteration = \
            np.squeeze(preprocessing.data_downsampling_crop([inverse_loss_mask_train_iteration], "numpy",
                                                            window_input_shape)[0])

        current_x_prediction = x_prediction * inverse_loss_mask_train_iteration
    else:
        current_x_prediction = x_prediction

    if gt_prediction is not None:
        gt_prediction, _ = preprocessing.data_preprocessing([gt_prediction], "numpy", preprocessing_steps)
        gt_prediction = np.squeeze(preprocessing.data_downsampling_crop([gt_prediction], "numpy",
                                                                        window_input_shape)[0])

        if tf.reduce_sum(tf.cast(inverse_loss_mask_train_iteration, dtype=tf.float32)) > 0.0:
            current_gt_prediction = gt_prediction * inverse_loss_mask_train_iteration
        else:
            current_gt_prediction = gt_prediction

        current_output_accuracy = \
            [losses.correlation_coefficient_accuracy(current_gt_prediction, current_x_prediction).numpy(),
             losses.scale_accuracy(current_gt_prediction, current_x_prediction).numpy()]
    else:
        y_train_iteration, _ = preprocessing.data_preprocessing([y_train_iteration], "numpy", preprocessing_steps)
        y_train_iteration = np.squeeze(preprocessing.data_downsampling_crop([y_train_iteration], "numpy",
                                                                            window_input_shape)[0])

        if tf.reduce_sum(tf.cast(inverse_loss_mask_train_iteration, dtype=tf.float32)) > 0.0:
            current_y_train_iteration = y_train_iteration * inverse_loss_mask_train_iteration
        else:
            current_y_train_iteration = y_train_iteration

        current_output_accuracy = \
            [losses.correlation_coefficient_accuracy(current_y_train_iteration, current_x_prediction).numpy(),
             losses.scale_accuracy(current_y_train_iteration, current_x_prediction).numpy()]

    print("Output accuracy:\t{0:<20}".format(str(current_output_accuracy[0])))
    print("Output scale accuracy:\t{0:<20}".format(str(current_output_accuracy[1])))
    print("Output total accuracy:\t{0:<20}".format(str(np.mean([current_output_accuracy[0], current_output_accuracy[1]]))))

    print("Output uncertainty:\t{0:<20}".format(str(tf.math.reduce_mean(x_prediction_uncertainty_volume).numpy())))

    output_volume = nib.Nifti1Image(x_prediction, np.eye(4), nib.Nifti1Header())

    current_data_path = "{0}/{1}_output.nii.gz".format(current_output_path, str(j))
    nib.save(output_volume, current_data_path)

    output_uncertainty_volume = nib.Nifti1Image(x_prediction_uncertainty_volume, np.eye(4), nib.Nifti1Header())

    current_uncertainty_data_path = "{0}/{1}_uncertainty.nii.gz".format(current_output_path, str(j))
    nib.save(output_uncertainty_volume, current_uncertainty_data_path)

    return current_data_path, current_uncertainty_data_path


def output_patient_time_point_predictions(window_data_paths, example_data, windowed_full_input_axial_size,
                                          full_input_shape, current_output_path, current_output_prefix, i):
    print("output_patient_time_point_predictions")

    if len(window_data_paths) > 1:
        number_of_windows = int(np.ceil(full_input_shape[-1] / parameters.data_window_size)) + 1
        number_of_overlaps = number_of_windows - 1

        overlap_size = int(np.ceil((((parameters.data_window_size * number_of_windows) -
                                     full_input_shape[-1]) / number_of_overlaps)))
        overlap_index = int(np.ceil(parameters.data_window_size - overlap_size))

        output_ramp_filter_increasing = np.zeros((full_input_shape[0], full_input_shape[1], overlap_size))
        output_ramp_filter_increment = 1.0 / (overlap_size + 1.0)

        for l in range(overlap_size):
            output_ramp_filter_increasing[:, :, l] = output_ramp_filter_increment * (l + 1.0)

        output_ramp_filter_decreasing = np.flip(output_ramp_filter_increasing, axis=-1)

        output_arrays = []

        for l in range(number_of_windows):
            current_overlap_index = overlap_index * l

            current_data = nib.load(window_data_paths[l]).get_data()

            if l != 0:
                current_data[:, :, :overlap_size] = current_data[:, :, :overlap_size] * output_ramp_filter_increasing

            if l != number_of_windows - 1:
                current_data[:, :, -overlap_size:] = current_data[:, :, -overlap_size:] * output_ramp_filter_decreasing

            current_output_array = np.zeros((full_input_shape[0], full_input_shape[1], windowed_full_input_axial_size))
            current_output_array[:, :, current_overlap_index:current_overlap_index + parameters.data_window_size] = \
                current_data

            current_output_array[np.isclose(current_output_array, 0.0)] = np.nan

            output_arrays.append(current_output_array)

        output_array = np.nansum(np.asarray(output_arrays), axis=0)
        output_array = np.nan_to_num(output_array, copy=False)
    else:
        output_array = nib.load(window_data_paths[0]).get_data()

    output_array = np.squeeze(preprocessing.data_downsampling_crop([output_array], "numpy", full_input_shape)[0])

    example_data_header = example_data.get_header()

    output_array = np.squeeze(preprocessing.data_downsampling([output_array], "numpy",
                                                              example_data_header.get_data_shape())[0])

    output_volume = nib.Nifti1Image(output_array, example_data.get_affine(), example_data_header)

    current_data_path = "{0}/{1}_{2}.nii.gz".format(current_output_path, str(i), current_output_prefix)
    nib.save(output_volume, current_data_path)

    return current_data_path


def train_model():
    print("train_model")

    # get data and lables
    x, y, example_data, preprocessing_steps, full_input_shape, windowed_full_input_axial_size, window_input_shape, gt, data_mask, loss_mask = \
        get_preprocessed_train_data()

    with gzip.GzipFile(x[0], "r") as file:
        input_shape = np.load(file)[0].shape

    model, optimiser, loss = architecture.get_model_all(input_shape)
    model.summary()

    model_update_path = "{0}/model.pkl".format(parameters.model_path)

    if os.path.exists(model_update_path):
        print("Previous model found!")

        with open(model_update_path, "rb") as file:
            model.set_weights(pickle.load(file))

    print("Memory usage:\t{0}".format(str(get_model_memory_usage(1, model))))

    k.utils.plot_model(model, to_file="{0}/model.pdf".format(output_path), show_shapes=True, show_dtype=True,
                       show_layer_names=True, expand_nested=True)

    output_paths = []
    output_uncertainty_paths = []

    for i in range(len(y)):
        if parameters.new_model_patient_bool and not parameters.new_model_window_bool:
            if os.path.exists(model_update_path):
                print("Previous model found!")

                with open(model_update_path, "rb") as file:
                    model.set_weights(pickle.load(file))
            else:
                model = architecture.get_model(input_shape)

        if parameters.new_optimiser_patient_bool and not parameters.new_optimiser_window_bool:
            optimiser = architecture.get_optimiser()

        print("Patient/Time point:\t{0}".format(str(i)))

        patient_output_path = "{0}/{1}".format(output_path, str(i))

        # create output directory
        if not os.path.exists(patient_output_path):
            os.makedirs(patient_output_path, mode=0o770)

        with gzip.GzipFile(x[i], "r") as file:
            number_of_windows = np.load(file).shape[0]

        current_window_data_paths = []
        current_window_uncertainty_data_paths = []

        for j in range(number_of_windows):
            if parameters.new_model_window_bool:
                if os.path.exists(model_update_path):
                    print("Previous model found!")

                    with open(model_update_path, "rb") as file:
                        model.set_weights(pickle.load(file))
                else:
                    model = architecture.get_model(input_shape)

            if parameters.new_optimiser_window_bool:
                optimiser = architecture.get_optimiser()

            print("Window:\t{0}".format(str(j)))

            window_output_path = "{0}/{1}".format(patient_output_path, str(j))

            # create output directory
            if not os.path.exists(window_output_path):
                os.makedirs(window_output_path, mode=0o770)

            plot_output_path = "{0}/plots".format(window_output_path)

            # create output directory
            if not os.path.exists(plot_output_path):
                os.makedirs(plot_output_path, mode=0o770)

            model_output_path = "{0}/models".format(window_output_path)

            # create output directory
            if not os.path.exists(model_output_path):
                os.makedirs(model_output_path, mode=0o770)

            with gzip.GzipFile(x[i], "r") as file:
                x_train_iteration = np.asarray([np.load(file)[j]])

            with gzip.GzipFile(y[i], "r") as file:
                y_train_iteration = np.asarray([np.load(file)[j]])

            if float_sixteen_bool:
                x_train_iteration = x_train_iteration.astype(np.float16)
                y_train_iteration = y_train_iteration.astype(np.float16)
            else:
                x_train_iteration = x_train_iteration.astype(np.float32)
                y_train_iteration = y_train_iteration.astype(np.float32)

            x_train_iteration = tf.convert_to_tensor(x_train_iteration)
            y_train_iteration = tf.convert_to_tensor(y_train_iteration)

            if gt is not None:
                with gzip.GzipFile(gt[i], "r") as file:
                    gt_prediction = np.asarray([np.load(file)[j]])

                if float_sixteen_bool:
                    gt_prediction = gt_prediction.astype(np.float16)
                else:
                    gt_prediction = gt_prediction.astype(np.float32)

                gt_prediction = tf.convert_to_tensor(gt_prediction)
            else:
                gt_prediction = None

            if loss_mask is not None:
                with gzip.GzipFile(loss_mask[i], "r") as file:
                    loss_mask_train_iteration = np.asarray([np.load(file)[j]])

                if float_sixteen_bool:
                    loss_mask_train_iteration = loss_mask_train_iteration.astype(np.float16)
                else:
                    loss_mask_train_iteration = loss_mask_train_iteration.astype(np.float32)

                loss_mask_train_iteration = tf.convert_to_tensor(loss_mask_train_iteration)
            else:
                loss_mask_train_iteration = None

            total_iterations = 0

            loss_list = []
            evaluation_loss_list = []
            previous_model_weight_list = []
            previous_optimiser_weight_list = []

            current_accuracy_nan_patience = 0

            total_gt_current_accuracy = None
            max_accuracy = tf.cast(0.0, dtype=tf.float32)
            max_accuracy_iteration = 0

            x_output = None
            y_output = None
            x_prediction_uncertainty_volume = None

            while True:
                if current_accuracy_nan_patience >= parameters.patience:
                    break

                print("Iteration:\t{0:<20}\tTotal iterations:\t{1:<20}".format(str(len(loss_list)), str(total_iterations)))

                model, loss_list, uncertainty, previous_model_weight_list, previous_optimiser_weight_list = \
                    train_step(model, optimiser, loss, x_train_iteration, y_train_iteration, loss_mask_train_iteration,
                               loss_list, previous_model_weight_list, previous_optimiser_weight_list,
                               model_output_path)

                x_prediction, evaluation_loss_list, x_prediction_uncertainty, x_prediction_uncertainty_volume, current_accuracy = \
                    test_step(model, loss, x_train_iteration, y_train_iteration, loss_mask_train_iteration,
                              evaluation_loss_list)

                if ((np.isnan(current_accuracy[0].numpy()) or np.isinf(current_accuracy[0].numpy())) or
                        (np.isnan(current_accuracy[1].numpy()) or np.isinf(current_accuracy[1].numpy()))):
                    model, optimiser, loss_list, evaluation_loss_list, previous_model_weight_list, previous_optimiser_weight_list = \
                        test_backup(model, optimiser, loss_list, evaluation_loss_list, previous_model_weight_list,
                                    previous_optimiser_weight_list)

                    current_accuracy_nan_patience = current_accuracy_nan_patience + 1

                    continue
                else:
                    if gt is None:
                        current_accuracy_nan_patience = 0

                total_current_accuracy = tf.math.reduce_mean([current_accuracy[0], current_accuracy[1]])

                print("Loss:\t{0:<20}\tAccuracy:\t{1:<20}\tScale accuracy:\t{2:<20}\tTotal accuracy:\t{3:<20}\tUncertainty:\t{4:<20}".format(str(loss_list[-1].numpy()), str(current_accuracy[0].numpy()), str(current_accuracy[1].numpy()), str(total_current_accuracy.numpy()), str(uncertainty.numpy())))

                if gt is not None:
                    inverse_loss_mask_train_iteration = (loss_mask_train_iteration * -1.0) + 1.0

                    if tf.reduce_sum(tf.cast(inverse_loss_mask_train_iteration, dtype=tf.float32)) > 0.0:
                        current_gt_prediction = gt_prediction * inverse_loss_mask_train_iteration
                        current_x_prediction = x_prediction * inverse_loss_mask_train_iteration
                    else:
                        current_gt_prediction = gt_prediction
                        current_x_prediction = x_prediction

                    gt_accuracy = [losses.correlation_coefficient_accuracy(current_gt_prediction, current_x_prediction),
                                   losses.scale_accuracy(current_gt_prediction, current_x_prediction)]

                    if ((np.isnan(gt_accuracy[0].numpy()) or np.isinf(gt_accuracy[0].numpy())) or
                            (np.isnan(gt_accuracy[1].numpy()) or np.isinf(gt_accuracy[1].numpy()))):
                        model, optimiser, loss_list, evaluation_loss_list, previous_model_weight_list, previous_optimiser_weight_list = \
                            test_backup(model, optimiser, loss_list, evaluation_loss_list, previous_model_weight_list,
                                        previous_optimiser_weight_list)

                        current_accuracy_nan_patience = current_accuracy_nan_patience + 1

                        continue
                    else:
                        current_accuracy_nan_patience = 0

                    total_gt_current_accuracy = tf.math.reduce_mean([gt_accuracy[0], gt_accuracy[1]])

                    print("GT loss:\t{0:<20}\tGT accuracy:\t{1:<20}\tGT scale accuracy:\t{2:<20}\tGT total accuracy:\t{3:<20}\tGT uncertainty:\t{4:<20}".format(str(evaluation_loss_list[-1].numpy()), str(gt_accuracy[0].numpy()), str(gt_accuracy[1].numpy()), str(total_gt_current_accuracy.numpy()), str(x_prediction_uncertainty.numpy())))

                    if max_accuracy < total_gt_current_accuracy:
                        max_accuracy = total_gt_current_accuracy
                        max_accuracy_iteration = len(loss_list)
                else:
                    if max_accuracy < total_current_accuracy:
                        max_accuracy = total_current_accuracy
                        max_accuracy_iteration = len(loss_list)

                if np.isnan(evaluation_loss_list[-1].numpy()):
                    model, optimiser, loss_list, evaluation_loss_list, previous_model_weight_list, previous_optimiser_weight_list = \
                        test_backup(model, optimiser, loss_list, evaluation_loss_list, previous_model_weight_list,
                                    previous_optimiser_weight_list)

                    evaluation_loss_list.pop()

                    continue

                x_output = np.squeeze(x_prediction.numpy()).astype(np.float64)
                y_output = np.squeeze(y_train_iteration.numpy()).astype(np.float64)

                plt.figure()

                if gt is not None:
                    plt.subplot(1, 3, 1)
                else:
                    plt.subplot(1, 2, 1)

                plt.imshow(x_output[:, :, int(x_output.shape[2] / 2)], cmap="Greys")

                if gt is not None:
                    plt.subplot(1, 3, 2)
                else:
                    plt.subplot(1, 2, 2)

                plt.imshow(y_output[:, :, int(y_output.shape[2] / 2)], cmap="Greys")

                if gt is not None:
                    gt_plot = np.squeeze(gt_prediction).astype(np.float64)

                    plt.subplot(1, 3, 3)
                    plt.imshow(gt_plot[:, :, int(gt_plot.shape[2] / 2)], cmap="Greys")

                plt.tight_layout()
                plt.savefig("{0}/{1}.png".format(plot_output_path, str(len(loss_list))), format="png", dpi=600,
                            bbox_inches="tight")
                plt.close()

                plot_files = os.listdir(plot_output_path)

                for l in range(len(plot_files)):
                    current_plot_file = plot_files[l].strip()

                    split_array = current_plot_file.split(".png")

                    if len(split_array) >= 2:
                        if int(split_array[0]) > len(loss_list):
                            os.remove("{0}/{1}".format(plot_output_path, current_plot_file))

                model_files = os.listdir(model_output_path)

                for l in range(len(model_files)):
                    current_plot_file = model_files[l].strip()

                    split_array = current_plot_file.split("model_")[-1]
                    split_array = split_array.split("optimiser_")[-1]

                    split_array = split_array.split(".pkl")

                    if len(split_array) >= 2:
                        if int(split_array[0]) > len(loss_list):
                            os.remove("{0}/{1}".format(model_output_path, current_plot_file))

                evaluation_loss_list_len = len(evaluation_loss_list)

                if evaluation_loss_list_len > 1 and evaluation_loss_list_len >= parameters.patience:
                    current_patience = parameters.patience

                    if current_patience < 2:
                        current_patience = 2

                    loss_gradient = np.gradient(evaluation_loss_list[-current_patience:])[-parameters.patience:]

                    print("Plateau cutoff:\t{0:<20}\tMax distance to plateau cutoff:\t{1:<20}".format(str(parameters.plateau_cutoff), str(np.max(np.abs(loss_gradient) - parameters.plateau_cutoff))))

                    if np.allclose(loss_gradient, np.zeros(loss_gradient.shape), rtol=0.0,
                                   atol=parameters.plateau_cutoff):
                        print("Reached plateau: Exiting...")
                        print("Maximum accuracy:\t{0:<20}\tMaximum accuracy iteration:\t{1:<20}".format(str(max_accuracy.numpy()), str(max_accuracy_iteration)))

                        if gt is not None:
                            accuracy_loss = np.abs(total_gt_current_accuracy - max_accuracy)
                        else:
                            accuracy_loss = np.abs(total_current_accuracy - max_accuracy)

                        print("Accuracy loss:\t{0:<20}".format(str(accuracy_loss)))

                        break

                total_iterations = total_iterations + 1

            if current_accuracy_nan_patience >= parameters.patience:
                print("Error: Input not suitable; continuing")

                continue

            if parameters.bayesian_test_bool != parameters.bayesian_output_bool:
                x_prediction, x_prediction_uncertainty, x_prediction_uncertainty_volume = \
                    get_bayesian_test_prediction(model, x_train_iteration, parameters.bayesian_iterations,
                                                 parameters.bayesian_output_bool)

                x_output = np.squeeze(x_prediction.numpy()).astype(np.float64)

            if gt_prediction is not None:
                gt_prediction = gt_prediction.numpy()

            loss_mask_train_iteration = np.squeeze(loss_mask_train_iteration.numpy()).astype(np.float64)

            current_window_data_path, current_window_uncertainty_data_path = \
                output_window_predictions(x_output, y_output, gt_prediction, loss_mask_train_iteration,
                                          np.squeeze(x_prediction_uncertainty_volume.numpy()).astype(np.float64),
                                          preprocessing_steps, window_input_shape, window_output_path, j)

            current_window_data_paths.append(current_window_data_path)
            current_window_uncertainty_data_paths.append(current_window_uncertainty_data_path)

            if parameters.new_model_window_bool:
                with open("{0}/model.pkl".format(window_output_path), "wb") as file:
                    pickle.dump(model.get_weights(), file)

                with open("{0}/optimiser.pkl".format(window_output_path), "wb") as file:
                    pickle.dump(optimiser.get_weights(), file)

        try:
            output_paths.append(output_patient_time_point_predictions(current_window_data_paths, example_data,
                                                                      windowed_full_input_axial_size,
                                                                      full_input_shape, patient_output_path, "output",
                                                                      i))

            output_uncertainty_paths.append(output_patient_time_point_predictions(current_window_uncertainty_data_paths,
                                                                                  example_data,
                                                                                  windowed_full_input_axial_size,
                                                                                  full_input_shape, patient_output_path,
                                                                                  "uncertainty", i))

            if parameters.new_model_patient_bool and not parameters.new_model_window_bool:
                with open("{0}/model.pkl".format(patient_output_path), "wb") as file:
                    pickle.dump(model.get_weights(), file)

                with open("{0}/optimiser.pkl".format(patient_output_path), "wb") as file:
                    pickle.dump(optimiser.get_weights(), file)
        except:
            print("Error: Input not suitable; continuing")

    if not parameters.new_model_patient_bool and not parameters.new_model_window_bool:
        with open("{0}/model.pkl".format(output_path), "wb") as file:
            pickle.dump(model.get_weights(), file)

        with open("{0}/optimiser.pkl".format(output_path), "wb") as file:
            pickle.dump(optimiser.get_weights(), file)

    return output_paths, output_uncertainty_paths


def main(input_data_path=None, input_output_path=None):
    print("main")

    global data_path
    global output_path

    if input_data_path is not None:
        data_path = input_data_path
    else:
        data_path = parameters.data_path

    if input_output_path is not None:
        output_path = input_output_path
    else:
        output_path = parameters.output_path

    # if debugging, remove previous output directory
    if input_output_path is None:
        if os.path.exists(output_path):
            shutil.rmtree(output_path)

    # create output directory
    if not os.path.exists(output_path):
        os.makedirs(output_path, mode=0o770)

    # create log file and begin writing to it
    logfile_path = "{0}/logfile.log".format(output_path)

    if os.path.exists(logfile_path):
        os.remove(logfile_path)

    import transcript
    logfile = transcript.transcript_start(logfile_path)

    output_paths, output_uncertainty_paths = train_model()

    # import python_email_notification
    # python_email_notification.main()

    device.reset()

    transcript.transcript_stop(logfile)

    return output_paths, output_uncertainty_paths


if __name__ == "__main__":
    main()
